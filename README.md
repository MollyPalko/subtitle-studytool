# subtitle-studytool

## requirements
```
pip install konlpy pysrt
```

## pipeline stages/passes
1. pass 1:
- transform srt files to json files
- do line by line processing
- do tokenization, lemmatization, pos tagging using konlpy
2. pass 2:
- insert json files to database entries
- insert TOPIK levels, frequency counts, etc

## directory structure (to have by the end of the project)
```
korean-subtitle-nlp/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ raw/                         # âœ… Manually & automatically collected SRTs
â”‚   â”œâ”€â”€ youtube/
â”‚   â”œâ”€â”€ dramas/
â”‚   â”œâ”€â”€ varietyShows/
â”‚   â””â”€â”€ idolLives/
â”‚
â”œâ”€â”€ data_aux/                    # âœ… Auxiliary data like word lists, dictionaries, etc.
â”‚   â”œâ”€â”€ topik_vocab.csv
â”‚   â”œâ”€â”€ korean_dict.csv
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data_intermediate/          # âœ… JSONL or CSV outputs from Pass 1
â”‚   â”œâ”€â”€ youtube/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data_db/                     # âœ… Optional: database file(s), e.g. SQLite
â”‚   â””â”€â”€ corpus.sqlite
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline/                # âœ… Core NLP pipeline logic
â”‚   â”‚   â”œâ”€â”€ pass1_tokenize.py
â”‚   â”‚   â”œâ”€â”€ pass2_index.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ nlp_helpers.py
â”‚   â”‚       â”œâ”€â”€ file_utils.py
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ downloaders/            # âœ… Subtitle download scripts
â”‚       â”œâ”€â”€ download_viki.py
â”‚       â”œâ”€â”€ download_yt.py
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ notebooks/                  # ğŸ““ Optional: Jupyter notebooks for testing/analyzing
â”‚   â””â”€â”€ explore_word_freq.ipynb
â”‚
â””â”€â”€ scripts/                    # ğŸ” CLI tools, DB setup, batch runners
    â”œâ”€â”€ run_pipeline.py
    â””â”€â”€ build_database.py
```


## database schema
    Words(
        word_id INTEGER PRIMARY KEY AUTOINCREMENT,
        word TEXT NOT NULL,
        pos_tag TEXT NOT NULL,
        topik_level INTEGER NOT NULL,
        homonym BOOLEAN DEFAULT 0,
        guide TEXT,
        UNIQUE(word, pos, level, guide)
    )
    Videos( 
        video_id INTEGER PRIMARY KEY AUTOINCREMENT,
        video_name TEXT NOT NULL,
        series_name TEXT,
        category TEXT,
        source_id TEXT            
        source TEXT
    )
    WordFrequency(
        word_id INTEGER,
        video_id INTEGER,
        frequency INTEGER DEFAULT 1,
        PRIMARY KEY (word_id, video_id),
        FOREIGN KEY (word_id) REFERENCES Words(word_id),
        FOREIGN KEY (video_id) REFERENCES Videos(video_id)
    )


### Note:
- user will simply be responsible for determining homonyms in context
- our analysis and database will track unique words in the sense of unique in terms of level, pos, and actual word.
- ex. ëˆˆ and ëˆˆ (both 1ê¸‰, ëª…ì‚¬) will not be differentiated in our database. it will enhance the learning experience of users to differentiate 'eye' and 'snow' in context
- we can consider fixing this or adding better homonym detection/differntiation in later versions
- words detected as homonyms in the words database will have the boolean homonym flag to aid users in detecting homonyms


## possibly helpful aux data
- TOPIK guide's 6000-word list
- GitHub versions in csv format
- Korean WordNet (KorLex) - great for root/semantic mappings, synonyms, etc
- NIADic (Korean Morpheme Dictionary from NIA)
- OpenKorDict

## future steps:
- create srt sub downloader for yt, viki, (and maybe weverse)
- create scripts to to use srt sub downloader to automatically populate raw directory
- improve pipeline in future iterations (confidence metrics, homonym differentiation, semantic analysis, automatic translation, etc)
- create GUI/CLI tools for on demand searching of YT and interacting with database
- integrate with Anki or other user data metrics


## current accuracy (manually graded by me)
```
ğŸ“Š Annotation Report for True_Beuty_ep1_annotated.jsonl
Total lines annotated: 140/953 (14.7%)
Correct (âœ…):     105 (75.0%)
Incorrect (âŒ):   35 (25.0%)
Skipped (â­):     0 (0.0%)

ğŸ“Š Annotation Report for BTS_VLOG_RM_ë¯¸ìˆ ê´€_annotated.jsonl
Total lines annotated: 169/410 (41.2%)
Correct (âœ…):     128 (75.7%)
Incorrect (âŒ):   38 (22.5%)
Skipped (â­):     3 (1.8%)
```

## current files and directory structure and purpose
/src/pipeline/
- annotate_output.py (original file to go through original output json and create another annotated json of correct/incorrect and notes)
- report_annotations.py (summarizes metrics like above from annotated json files)
- resume_annotations.py (better script to annotate output jsons for correctness - picks up where you left off last time)
- srt_to_json.py (contains function that takes in paths to input srt and output json and does the pass 1)

/json/
- BTS_VLOG_RM_ë¯¸ìˆ ê´€_annotated.jsonl
- BTS_VLOG_RM_ë¯¸ìˆ ê´€.jsonl
- Coffee_Prince_ep1.jsonl
- True_Beuty_ep1_annotated.jsonl
- True_Beuty_ep1.jsonl

the scheme here is that the original output data from pass 1 is the *.jsonl file and my manually annotated files are the *_annotated.jsonl files


## /aux_data/topik/
- total of 10635 words are labeled 1ê¸‰-6ê¸‰ across 6 files
- these files downloaded from [kleocean](https://kleocean.com/í† í”½-ì–´íœ˜-topik-vocab/)
- files from kleocean for topik I and II lists were passed up for containing only 1847 and 3873 words respectively and only labeling words as ì´ˆ/ì¤‘ not 1-6
- file from [êµ­ë¦½êµ­ì–´ì›](https://www.korean.go.kr/front/etcData/etcDataView.do?mn_id=46&etc_seq=71&pageIndex=21) were passed up for scoring words by A/B/C only and for containing only 5966 words. (this resource does contain hanja in addition to pos tagging though)
- english pos column is added when inserted to database, new word_id is created to be key, and usage column is eliminated



## how to insert parsed words from json into the database:
first in the sqlite shell, add the video entry:
```
INSERT INTO Videos (video_name, category)
VALUES ('[BTS VLOG] RM | ë¯¸ìˆ ê´€ VLOG', 'YouTube');
```

get the video id of newly added entry:
```
SELECT video_id, video_name 
FROM Videos WHERE video_name = '[BTS VLOG] RM | ë¯¸ìˆ ê´€ VLOG';
```

then in the cmd line, run the script to add the words and get the summary:
```
~/subtitle-studytool/src/database$ python process_tokens.py ../../json/BTS_VLOG_RM_ë¯¸ìˆ ê´€.jsonl --db korean_vocab.db --video-id 1
=== SUMMARY ===
Processed file:         BTS_VLOG_RM_ë¯¸ìˆ ê´€.jsonl
Total tokens:           2462
Matched tokens:         1132
Ignored tokens:         706
Unmatched tokens:       624
Video ID:               1
WordFrequency updated:  447 words
Logs saved:             ignored_tokens_BTS_VLOG_RM_ë¯¸ìˆ ê´€.txt, unmatched_tokens_BTS_VLOG_RM_ë¯¸ìˆ ê´€.txt
```


