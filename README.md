# subtitle-studytool

## requirements
```
pip install konlpy pysrt
```

## pipeline stages/passes
1. pass 1:
- transform srt files to json files
- do line by line processing
- do tokenization, lemmatization, pos tagging using konlpy
2. pass 2:
- insert json files to database entries
- insert TOPIK levels, frequency counts, etc

## directory structure (to have by the end of the project)
korean-subtitle-nlp/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ raw/                         # âœ… Manually & automatically collected SRTs
â”‚   â”œâ”€â”€ youtube/
â”‚   â”œâ”€â”€ dramas/
â”‚   â”œâ”€â”€ varietyShows/
â”‚   â””â”€â”€ idolLives/
â”‚
â”œâ”€â”€ data_aux/                    # âœ… Auxiliary data like word lists, dictionaries, etc.
â”‚   â”œâ”€â”€ topik_vocab.csv
â”‚   â”œâ”€â”€ korean_dict.csv
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data_intermediate/          # âœ… JSONL or CSV outputs from Pass 1
â”‚   â”œâ”€â”€ youtube/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data_db/                     # âœ… Optional: database file(s), e.g. SQLite
â”‚   â””â”€â”€ corpus.sqlite
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline/                # âœ… Core NLP pipeline logic
â”‚   â”‚   â”œâ”€â”€ pass1_tokenize.py
â”‚   â”‚   â”œâ”€â”€ pass2_index.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ nlp_helpers.py
â”‚   â”‚       â”œâ”€â”€ file_utils.py
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ downloaders/            # âœ… Subtitle download scripts
â”‚       â”œâ”€â”€ download_viki.py
â”‚       â”œâ”€â”€ download_yt.py
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ notebooks/                  # ğŸ““ Optional: Jupyter notebooks for testing/analyzing
â”‚   â””â”€â”€ explore_word_freq.ipynb
â”‚
â””â”€â”€ scripts/                    # ğŸ” CLI tools, DB setup, batch runners
    â”œâ”€â”€ run_pipeline.py
    â””â”€â”€ build_database.py


## database schema
    Words(
        word_id INTEGER PRIMARY KEY,
        word TEXT NOT NULL,
        pos_tag TEXT,             -- e.g., Noun, Verb, Adjective
        topik_level INTEGER       -- 1â€“6 or NULL if unknown
    )
    Videos( 
        video_id INTEGER PRIMARY KEY,
        video_name TEXT NOT NULL,
        series_name TEXT,         -- NULL if not 
        category TEXT,            -- 'drama', 'variety', 'idol_live', 'youtube'
        source_id TEXT            
        source TEXT
    )
    WordFrequency(
        word_id INTEGER,
        video_id INTEGER,
        frequency INTEGER DEFAULT 1,
        PRIMARY KEY (word_id, video_id),
        FOREIGN KEY (word_id) REFERENCES Words(word_id),
        FOREIGN KEY (video_id) REFERENCES Videos(video_id)
    )

## possibly helpful aux data
- TOPIK guide's 6000-word list
- GitHub versions in csv format
- Korean WordNet (KorLex) - great for root/semantic mappings, synonyms, etc
- NIADic (Korean Morpheme Dictionary from NIA)
- OpenKorDict

## future steps:
- create srt sub downloader for yt, viki, (and maybe weverse)
- create scripts to to use srt sub downloader to automatically populate raw directory
- improve pipeline in future iterations (confidence metrics, homonym differentiation, semantic analysis, automatic translation, etc)
- create GUI/CLI tools for on demand searching of YT and interacting with database
- integrate with Anki or other user data metrics


## current accuracy (manually graded by me)
ğŸ“Š Annotation Report for True_Beuty_ep1_annotated.jsonl
Total lines annotated: 140/953 (14.7%)
Correct (âœ…):     105 (75.0%)
Incorrect (âŒ):   35 (25.0%)
Skipped (â­):     0 (0.0%)

ğŸ“Š Annotation Report for BTS_VLOG_RM_ë¯¸ìˆ ê´€_annotated.jsonl
Total lines annotated: 169/410 (41.2%)
Correct (âœ…):     128 (75.7%)
Incorrect (âŒ):   38 (22.5%)
Skipped (â­):     3 (1.8%)


## current files and directory structure and purpose
/src/pipeline/
- annotate_output.py (original file to go through original output json and create another annotated json of correct/incorrect and notes)
- report_annotations.py (summarizes metrics like above from annotated json files)
- resume_annotations.py (better script to annotate output jsons for correctness - picks up where you left off last time)
- srt_to_json.py (contains function that takes in paths to input srt and output json and does the pass 1)

/json/
- BTS_VLOG_RM_ë¯¸ìˆ ê´€_annotated.jsonl
- BTS_VLOG_RM_ë¯¸ìˆ ê´€.jsonl
- Coffee_Prince_ep1.jsonl
- True_Beuty_ep1_annotated.jsonl
- True_Beuty_ep1.jsonl

the scheme here is that the original output data from pass 1 is the *.jsonl file and my manually annotated files are the *_annotated.jsonl files




